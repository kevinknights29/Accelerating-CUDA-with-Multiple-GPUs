{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#65AE11;\">Multiple GPUs</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA can manage multiple GPU devices on a single host. In this section you will learn how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Objectives</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this section you will understand the CUDA syntax to:\n",
    "\n",
    "* Get how many GPUs are available to your application\n",
    "* Activate any of the available GPUs\n",
    "* Allocate memory on multiple GPUs\n",
    "* Transfer memory to and from multiple GPUs\n",
    "* Launch kernels on multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Getting Information About Multiple GPUs</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To computationally obtain the number of available GPUs available use `cudaGetDeviceCount`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "int num_gpus;\n",
    "cudaGetDeviceCount(&num_gpus);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To programmatically obtain the currently active GPU use `cudaGetDevice`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "int device;\n",
    "cudaGetDevice(&device); // `device` is now a 0-based index of the current GPU.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Setting the Current GPU</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each host thread, one GPU device is active at a time. To set a specific GPU as active use `cudaSetDevice` with the desired GPU's 0-based index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "cudaSetDevice(0);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Looping Over Available GPUs</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common pattern is to loop over available GPUs, performing operations for each:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "int num_gpus;\n",
    "cudaGetDeviceCount(&num_gpus);\n",
    "\n",
    "for (int gpu = 0; gpu < num_gpus; gpu++) {\n",
    "\n",
    "    cudaSetDevice(gpu);\n",
    "    \n",
    "    // Perform operations for this GPU.\n",
    "}    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Data Chunking for Multiple GPUs</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*If you need to review how to do robust indexing for data chunks, please see [Copy Compute Considerations](../08_Copy_Compute_Considerations/Copy_Compute_Considerations.ipynb).*)\n",
    "\n",
    "As with multiple non-default streams, each of multiple GPUs can work with a chunk of data. Here we create and utilize an array of data pointers to allocate memory for each available GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "const int num_gpus;\n",
    "cudaGetDeviceCount(&num_gpus);\n",
    "\n",
    "const uint64_t num_entries = 1UL << 26;\n",
    "const uint64_t chunk_size = sdiv(num_entries, num_gpus);\n",
    "\n",
    "uint64_t *data_gpu[num_gpus]; // One pointer for each GPU.\n",
    "\n",
    "for (int gpu = 0; gpu < num_gpus; gpu++) {\n",
    "\n",
    "    cudaSetDevice(gpu);\n",
    "\n",
    "    const uint64_t lower = chunk_size*gpu;\n",
    "    const uint64_t upper = min(lower+chunk_size, num_entries);\n",
    "    const uint64_t width = upper-lower;\n",
    "\n",
    "    cudaMalloc(&data_gpu[gpu], sizeof(uint64_t)*width); // Allocate chunk of data for current GPU.\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Data Copies for Multiple GPUs</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same looping and chunking techniques, data can be transfered to and from multiple GPUs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "// ...Assume data has been allocated on host and for each GPU\n",
    "\n",
    "for (int gpu = 0; gpu < num_gpus; gpu++) {\n",
    "\n",
    "    cudaSetDevice(gpu);\n",
    "\n",
    "    const uint64_t lower = chunk_size*gpu;\n",
    "    const uint64_t upper = min(lower+chunk_size, num_entries);\n",
    "    const uint64_t width = upper-lower;\n",
    "\n",
    "    // Note use of `cudaMemcpy` and not `cudaMemcpyAsync` since we are not\n",
    "    // presently using non-default streams.\n",
    "    cudaMemcpy(data_gpu[gpu], data_cpu+lower, \n",
    "           sizeof(uint64_t)*width, cudaMemcpyHostToDevice); // ...or cudaMemcpyDeviceToHost\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Kernel Launches for Multiple GPUs</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same looping and chunking techniques, kernels can be launched to work on chunks of data on multiple GPUs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "// ...Assume data has been allocated on host and for each GPU\n",
    "\n",
    "for (int gpu = 0; gpu < num_gpus; gpu++) {\n",
    "\n",
    "    cudaSetDevice(gpu);\n",
    "\n",
    "    const uint64_t lower = chunk_size*gpu;\n",
    "    const uint64_t upper = min(lower+chunk_size, num_entries);\n",
    "    const uint64_t width = upper-lower;\n",
    "\n",
    "    kernel<<<grid, block>>>(data_gpu[gpu], width); // Pass chunk of data for current GPU to work on.\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Check for Understanding</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer the following to confirm you've learned the main objectives of this section. You can display the answers for each question by clicking on the \"...\" cells below the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What CUDA Runtime call tells us how many GPUs are available?**\n",
    "\n",
    "1. `cudaGetDevice`\n",
    "2. `cudaSetDevice`\n",
    "3. `cudaGetDeviceCount`\n",
    "4. `cudaGetDeviceProperties`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Click to Show Solution\n",
    "\n",
    "**Answer: 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What CUDA Runtime selects a GPU to be currently active?**\n",
    "\n",
    "1. `cudaGetDevice`\n",
    "2. `cudaSetDevice`\n",
    "3. `cudaGetDeviceCount`\n",
    "4. `cudaGetDeviceProperties`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Click to Show Solution\n",
    "\n",
    "**Answer: 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What would the index be for a GPU on a single-GPU system?**\n",
    "\n",
    "1. 1\n",
    "2. 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Click to Show Solution\n",
    "\n",
    "**Answer: 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Next</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are familiar with the syntax and techniques for utilizing multiple GPUs, you will, in the next section, apply your understanding to refactor the baseline cipher to use multiple GPUs.\n",
    "\n",
    "Please continue to the next section: [*Exercise: MGPU*](../11_Exercise_MGPU/Exercise_MGPU.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Optional Further Study</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are for students with time and interest to do additional study on topics related to this workshop.\n",
    "\n",
    "* In the above, we utlize a [depth-first](https://www.geeksforgeeks.org/difference-between-bfs-and-dfs/#:~:text=BFS(Breadth%20First%20Search)%20uses,edges%20from%20a%20source%20vertex.) approach to pass chunks of work to each GPU. In some scenarios, especially when the amount of data is extreme, it may make much more sense to utilize a breadth-first approach. This change in approach is not somehing that requires additional CUDA knowledge, but, this [stack overflow answer](https://stackoverflow.com/questions/11673154/concurrency-in-cuda-multi-gpu-executions) provides several examples of CUDA code using both depth-first and breadth-first approaches.\n",
    "* Both peer to peer memory transfers between multiple GPUs, and, the use of multiple GPUs on multiple nodes are outside the scope of this workshop. [This Supercomputing Conference Presentation](https://www.nvidia.com/docs/IO/116711/sc11-multi-gpu.pdf) will give you a good starting point for exploring these topics (and more)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
